# Next-Word-Prediction

Project Idea : Develop a model that captures both syntactic and semantic relationships between words, generate accurate predictions for the next word in a sequence.

Modules included in our Project:
1. Data collection and preprocessing 
2. Train and test split 
3. Model Training
4.  Model selection
5. Integrating GUI

Image Preprocessing : 
The project begins with collection of plethora of textual data. The textual data is generated by creating an extra attribute from SWAG (Situations with Adversarial Generations).The start Phase and appropriate ending of start Phrase are combined to form a complete sentence and named as label. The textual data in label scrutinized as a corpus text. The corpus text is preprocessed by tokenizing text data. The tokenizer analyses the text and assigns a unique integer index to each word of text. The fit on texts method is called on tokenizer object to update the internal vocabulary based on the list of texts.

 Feature Extraction : 
Creating Input Sequences: The input sequences are created using sliding window approach n-gram. It iterates through tokenized words in each sentence and forms sequences of increasing lengths, from 2 to the length of the sentence. The sequences are padded to ensure that they all have same length. Padding is added to the beginning of each sequence, and the length is set to maximum sequence length. 
One-Hot Encoding Labels: The Associated labels of input sequences generated by sliding window approach are converted to a one-hot encoded format using to categorical from the Keras utilities. This is often done when training a neural network for a classification task with categorical labels.

Model Training : 
Text generation model using a Recurrent Neural Network (RNN) architecture with Long Short-Term Memory (LSTM) cells and Gated Recurrent Unit (GRU) are trained with generated text and labels. The BERT modelis trained by fine tuning. The last token of input sequence is replaced with a special [MASK] token, for MLM(Masked Language Modelling) to make a prediction at that position. The training objective is to minimize the difference between the predicted probability distribution of the masked tokens and the actual distribution of the true tokens.

 Model selection:
 inal model is determined based on the cormpassion of loss and accuracy of individual models . The loss function quantifies the difference between the predicted outputs of the model and the actual target outputs. The model predicts a probability distribution over the vocabulary for the next word, and the loss is calculated based on difference between this predicted value over the vocabulary for the next word.

Integrating GUI:
The final model is downloaded using joblib software to integrate it with the Graphical User Interface application developed using Streamlit open-source library for seamless user interaction and convenience.


